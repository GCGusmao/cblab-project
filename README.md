# Desafio de Engenharia de Dados - Coco Bambu

Este repositÃ³rio contÃ©m a soluÃ§Ã£o completa para o desafio tÃ©cnico do processo seletivo de Engenharia de Dados, proposto pelo CBLab.

O projeto aborda os dois principais desafios propostos:

1.  **Modelagem de Dados:** A transcriÃ§Ã£o de uma estrutura JSON (de um ERP) para um schema de banco de dados relacional robusto, incluindo a justificativa da abordagem.
2.  **Arquitetura de Dados:** O design de uma arquitetura para ingestÃ£o e armazenamento de dados de mÃºltiplas APIs em um Data Lake, considerando performance, escalabilidade e a evoluÃ§Ã£o do schema.

## ğŸ“œ Ãndice

  * [ğŸ“‚ Estrutura do Projeto](#estrutura-do-projeto)
  * [ğŸš€ Quickstart: Executando o Projeto](#quickstart-executando-o-projeto)
  * [ğŸ› ï¸ Conectando ao Banco de Dados](#conectando-ao-banco-de-dados)
  * [ğŸ“„ SoluÃ§Ã£o Detalhada do Desafio](#soluÃ§Ã£o-detalhada-do-desafio)

  * [SoluÃ§Ã£o Detalhada do Desafio](#soluÃ§Ã£o-detalhada-do-desafio)

## ğŸ“ Estrutura do Projeto

O repositÃ³rio estÃ¡ organizado da seguinte forma para garantir clareza e separaÃ§Ã£o de responsabilidades:

```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ERP.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ desafio_1/
â”‚   â”‚   â”œâ”€â”€ 1_1_descricao_esquema.md
â”‚   â”‚   â”œâ”€â”€ 1_2_schema.sql
â”‚   â”‚   â”œâ”€â”€ 1_3_modelagem_detalhada.md
â”‚   â”‚   â””â”€â”€ 1_3_parser.py
â”‚   â””â”€â”€ desafio_2/
â”‚       â”œâ”€â”€ 2_1_armazenamento_api.md
â”‚       â”œâ”€â”€ 2_2_estrutura_data_lake.md
â”‚       â””â”€â”€ 2_3_mudanca_schema.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ KANBAN.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Quickstart: Executando o Projeto

Para validar a soluÃ§Ã£o do Desafio 1 na prÃ¡tica, o projeto inclui um ambiente Docker que provisiona um banco de dados PostgreSQL e um script Python que realiza o parsing do JSON e a inserÃ§Ã£o dos dados.

**âš ï¸ AtenÃ§Ã£o:** a estrutura possui credenciais e variÃ¡veis de ambiente escritas diretamente no cÃ³digo. Essa abordagem tem como objetivo facilitar o processo de execuÃ§Ã£o e testes dessas soluÃ§Ãµes, mas **nunca deve ser utilizada em produÃ§Ã£o**. Sempre armazene informaÃ§Ãµes sensÃ­veis em arquivos de variÃ¡veis de ambiente, como '.env', e nunca publique essas soluÃ§Ãµes de forma pÃºblica.

### PrÃ©-requisitos

  * [Docker](https://www.google.com/search?q=https://www.docker.com/get-started) instalado e em execuÃ§Ã£o.
  * [Docker Compose](https://docs.docker.com/compose/install/) instalado.
  * Python 3.8+ instalado.

### Passo a Passo para ExecuÃ§Ã£o

1.  **Inicie o Ambiente Docker**
    Este comando irÃ¡ iniciar um contÃªiner PostgreSQL em segundo plano e executar automaticamente o script `1_2_schema.sql` para criar toda a estrutura de tabelas.

    ```bash
    docker-compose up -d
    ```

2.  **Instale as DependÃªncias Python**
    Crie um ambiente virtual (recomendado) e instale a biblioteca necessÃ¡ria.

    ```bash
    python -m venv venv
    source venv/bin/activate  # No Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```

3.  **Execute o Script de Parser**
    Este script irÃ¡ ler o arquivo `data/ERP.json`, conectar-se ao banco de dados no Docker e inserir os dados, validando a modelagem.

    ```bash
    python src/desafio_1/1_3_parser.py
    ```

    Se a execuÃ§Ã£o for bem-sucedida, vocÃª verÃ¡ uma mensagem de sucesso no final, confirmando que a modelagem foi validada.

## ğŸ› ï¸ Conectando ao Banco de Dados

ApÃ³s iniciar o ambiente Docker, vocÃª pode se conectar ao banco de dados para inspecionar as tabelas e os dados inseridos pelo parser.

**1. Conectar via linha de comando (psql):**

```bash
docker exec -it coco-bambu-postgres-db psql -U cblab_user -d coco_bambu_db
```

Dentro do psql, use o comando `\dt` para listar as tabelas.

**2. Conectar via Ferramenta GrÃ¡fica (DBeaver, pgAdmin, etc.):**

  * **Host:** `localhost`
  * **Porta:** `5435`
  * **Banco de Dados:** `coco_bambu_db`
  * **UsuÃ¡rio:** `cblab_user`
  * **Senha:** `a_senha_super_segura` (a senha definida no `docker-compose.yml`)

## ğŸ“„ SoluÃ§Ã£o Detalhada do Desafio

As respostas detalhadas para cada item do desafio estÃ£o organizadas em arquivos Markdown dedicados dentro da pasta `src/`.

### Desafio 1: Modelagem de Dados

  * **1.1. DescriÃ§Ã£o do Esquema JSON:** A anÃ¡lise detalhada da estrutura, tipos de dados e hierarquia do arquivo `ERP.json` pode ser encontrada em:

      * [`src/desafio_1/1_1_descricao_esquema.md`](src/desafio_1/1_1_descricao_esquema.md)

  * **1.2. TranscriÃ§Ã£o para Tabelas SQL:** O script SQL completo e comentado para a criaÃ§Ã£o do schema relacional estÃ¡ em:

      * [`src/desafio_1/1_2_schema.sql`](src/desafio_1/1_2_schema.sql)

  * **1.3. DescriÃ§Ã£o da Abordagem:** A justificativa completa para as decisÃµes de design, incluindo a abordagem polimÃ³rfica, tratamento de dados temporais e integridade referencial, estÃ¡ detalhada em:

      * [`src/desafio_1/1_3_modelagem_detalhada.md`](src/desafio_1/1_3_modelagem_detalhada.md)

### Desafio 2: Arquitetura de Pipeline e Data Lake

  * **2.1. Por que armazenar as respostas das APIs?:** A discussÃ£o estratÃ©gica sobre a importÃ¢ncia de criar uma camada de dados brutos (Bronze), no contexto da Arquitetura MedalhÃ£o, estÃ¡ em:

      * [`src/desafio_2/2_1_armazenamento_api.md`](src/desafio_2/2_1_armazenamento_api.md)

  * **2.2. Estrutura de Armazenamento:** O design da estrutura de pastas em um Data Lake no AWS S3, com foco em particionamento estratÃ©gico para performance com AWS Athena, estÃ¡ detalhado para os 5 endpoints em:

      * [`src/desafio_2/2_2_estrutura_data_lake.md`](src/desafio_2/2_2_estrutura_data_lake.md)

  * **2.3. ImplicaÃ§Ãµes de MudanÃ§a no Schema:** A anÃ¡lise do cenÃ¡rio de evoluÃ§Ã£o de schema (`Schema Drift`) e as estratÃ©gias para construir um pipeline de dados resiliente estÃ£o em:

      * [`src/desafio_2/2_3_mudanca_schema.md`](src/desafio_2/2_3_mudanca_schema.md)

